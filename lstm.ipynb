{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yilmazyasin/hello-world/blob/master/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DfNO5l8nD2d"
      },
      "source": [
        "# Sun Spots Example\n",
        "\n",
        "This is an example of RNN regression to predict sunspots. The code below loads the dataset from a website. If needed, here is the main data source:\n",
        "\n",
        "* [Sunspot Data Files](http://www.sidc.be/silso/datafiles#total)\n",
        "* [Download Daily Sunspots](http://www.sidc.be/silso/INFO/sndtotcsv.php) - 1/1/1818 to now.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c561r7ZCnD2d",
        "outputId": "2b853ac2-745b-4a3f-8f74-4a636f995930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting file:\n",
            "   year  month  day  dec_year  sn_value  sn_error  obs_num  unused1\n",
            "0  1818      1    1  1818.001        -1       NaN        0        1\n",
            "1  1818      1    2  1818.004        -1       NaN        0        1\n",
            "2  1818      1    3  1818.007        -1       NaN        0        1\n",
            "3  1818      1    4  1818.010        -1       NaN        0        1\n",
            "4  1818      1    5  1818.012        -1       NaN        0        1\n",
            "5  1818      1    6  1818.015        -1       NaN        0        1\n",
            "6  1818      1    7  1818.018        -1       NaN        0        1\n",
            "7  1818      1    8  1818.021        65      10.2        1        1\n",
            "8  1818      1    9  1818.023        -1       NaN        0        1\n",
            "9  1818      1   10  1818.026        -1       NaN        0        1\n",
            "Ending file:\n",
            "       year  month  day  dec_year  sn_value  sn_error  obs_num  unused1\n",
            "72855  2017      6   21  2017.470        35       1.0       41        0\n",
            "72856  2017      6   22  2017.473        24       0.8       39        0\n",
            "72857  2017      6   23  2017.475        23       0.9       40        0\n",
            "72858  2017      6   24  2017.478        26       2.3       15        0\n",
            "72859  2017      6   25  2017.481        17       1.0       18        0\n",
            "72860  2017      6   26  2017.484        21       1.1       25        0\n",
            "72861  2017      6   27  2017.486        19       1.2       36        0\n",
            "72862  2017      6   28  2017.489        17       1.1       22        0\n",
            "72863  2017      6   29  2017.492        12       0.5       25        0\n",
            "72864  2017      6   30  2017.495        11       0.5       30        0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "  \n",
        "names = ['year', 'month', 'day', 'dec_year', 'sn_value' , \n",
        "         'sn_error', 'obs_num', 'unused1']\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/SN_d_tot_V2.0.csv\",\n",
        "    sep=';',header=None,names=names,\n",
        "    na_values=['-1'], index_col=False)\n",
        "\n",
        "print(\"Starting file:\")\n",
        "print(df[0:10])\n",
        "\n",
        "print(\"Ending file:\")\n",
        "print(df[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R3-vBANnD2d"
      },
      "source": [
        "As you can see, there is quite a bit of missing data near the end of the file.  We want to find the starting index where the missing data no longer occurs.  This technique is somewhat sloppy; it would be better to find a use for the data between missing values.  However, the point of this example is to show how to use LSTM with a somewhat simple time-series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT-4MgobnD2d",
        "outputId": "0920d2bf-e9a3-4fe4-fbfe-6b5dd46e4055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314\n"
          ]
        }
      ],
      "source": [
        "start_id = max(df[df['obs_num'] == 0].index.tolist())+1  # Find the last zero and move one beyond\n",
        "print(start_id)\n",
        "df = df[start_id:] # Trim the rows that have missing observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oL8-qB2nD2e",
        "outputId": "f9dfc259-41e3-4916-eb3a-c17cce6a875c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 55160 observations.\n",
            "Test set has 6391 observations.\n"
          ]
        }
      ],
      "source": [
        "df['sn_value'] = df['sn_value'].astype(float)\n",
        "df_train = df[df['year']<2000]\n",
        "df_test = df[df['year']>=2000]\n",
        "\n",
        "spots_train = df_train['sn_value'].tolist()\n",
        "spots_test = df_test['sn_value'].tolist()\n",
        "\n",
        "print(\"Training set has {} observations.\".format(len(spots_train)))\n",
        "print(\"Test set has {} observations.\".format(len(spots_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipM_LjwxnD2e"
      },
      "source": [
        "To create an algorithm that will predict future values, we need to consider how to encode this data to be presented to the algorithm. The data must be submitted as sequences, using a sliding window algorithm to encode the data. We must define how large the window will be. Consider an n-sized window. Each sequence's $x$ values will be a sequence of $n$ data points. The $y$'s will be the next value, after the sequence, that we are trying to predict. You can use the following function to take a series of values, such as sunspots, and generate sequences ($x$) and predicted values ($y$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyUII6JynD2e",
        "outputId": "ca422191-fcb4-4b1c-9a58-603c90a32fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training set: (55150, 10, 1)\n",
            "Shape of test set: (6381, 10, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_sequences(seq_size, obs):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(obs)-SEQUENCE_SIZE):\n",
        "        #print(i)\n",
        "        window = obs[i:(i+SEQUENCE_SIZE)]\n",
        "        after_window = obs[i+SEQUENCE_SIZE]\n",
        "        window = [[x] for x in window]\n",
        "        #print(\"{} - {}\".format(window,after_window))\n",
        "        x.append(window)\n",
        "        y.append(after_window)\n",
        "        \n",
        "    return np.array(x),np.array(y)\n",
        "    \n",
        "    \n",
        "SEQUENCE_SIZE = 10\n",
        "x_train,y_train = to_sequences(SEQUENCE_SIZE,spots_train)\n",
        "x_test,y_test = to_sequences(SEQUENCE_SIZE,spots_test)\n",
        "\n",
        "print(\"Shape of training set: {}\".format(x_train.shape))\n",
        "print(\"Shape of test set: {}\".format(x_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXxRjf2vnrKC"
      },
      "source": [
        "We can see the internal structure of the training data. The first dimension is the number of training elements, the second indicates a sequence size of 10, and finally, we have one data point per timeslice in the window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckobdirKnD2e",
        "outputId": "ce794e12-9136-4e99-b8c4-1d1829bf50fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55150, 10, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6_eY2f-nD2e"
      },
      "source": [
        "We are now ready to build and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3whCrzBQnD2e",
        "outputId": "d5a6b210-b75a-45b0-9005-3640226bf627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Epoch 1/1000\n",
            "1724/1724 - 13s - loss: 1296.0040 - val_loss: 241.7857 - 13s/epoch - 8ms/step\n",
            "Epoch 2/1000\n",
            "1724/1724 - 15s - loss: 513.9359 - val_loss: 213.6968 - 15s/epoch - 9ms/step\n",
            "Epoch 3/1000\n",
            "1724/1724 - 15s - loss: 506.0090 - val_loss: 209.2955 - 15s/epoch - 9ms/step\n",
            "Epoch 4/1000\n",
            "1724/1724 - 10s - loss: 505.2627 - val_loss: 271.9765 - 10s/epoch - 6ms/step\n",
            "Epoch 5/1000\n",
            "1724/1724 - 10s - loss: 505.0906 - val_loss: 210.6352 - 10s/epoch - 6ms/step\n",
            "Epoch 6/1000\n",
            "1724/1724 - 10s - loss: 503.4268 - val_loss: 206.9142 - 10s/epoch - 6ms/step\n",
            "Epoch 7/1000\n",
            "1724/1724 - 10s - loss: 500.3625 - val_loss: 220.0493 - 10s/epoch - 6ms/step\n",
            "Epoch 8/1000\n",
            "1724/1724 - 10s - loss: 499.8954 - val_loss: 214.0473 - 10s/epoch - 6ms/step\n",
            "Epoch 9/1000\n",
            "1724/1724 - 10s - loss: 500.4660 - val_loss: 214.1216 - 10s/epoch - 6ms/step\n",
            "Epoch 10/1000\n",
            "1724/1724 - 10s - loss: 498.1349 - val_loss: 222.3993 - 10s/epoch - 6ms/step\n",
            "Epoch 11/1000\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "1724/1724 - 10s - loss: 497.9560 - val_loss: 223.2399 - 10s/epoch - 6ms/step\n",
            "Epoch 11: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4602041150>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, dropout=0.0, recurrent_dropout=0.0,input_shape=(None, 1)))\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
        "                        verbose=1, mode='auto', restore_best_weights=True)\n",
        "print('Train...')\n",
        "\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "          callbacks=[monitor],verbose=2,epochs=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdKziq28nD2e"
      },
      "source": [
        "Finally, we evaluate the model with RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TspuM6gznD2e"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(\"Score (RMSE): {}\".format(score))"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "t81_558_class_10_2_lstm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}